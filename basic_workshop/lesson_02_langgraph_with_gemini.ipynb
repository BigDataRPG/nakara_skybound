{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Workshop: LangGraph with Google Gemini\n",
    "\n",
    "This workshop covers:\n",
    "- Introduction to Google Gemini\n",
    "- LangGraph with Gemini integration\n",
    "- Prompting techniques with Gemini\n",
    "- System prompts and personas\n",
    "- Building workflows with Gemini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1758154839.780101 3954261 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langgraph.graph import Graph, StateGraph\n",
    "from typing import TypedDict, List\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize Gemini LLM\n",
    "# Make sure to set GOOGLE_API_KEY in your .env file\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Setup\n",
    "\n",
    "Make sure to create a `.env` file with your Google API key:\n",
    "```\n",
    "GOOGLE_API_KEY=your_google_api_key_here\n",
    "```\n",
    "\n",
    "You can get your API key from: https://makersuite.google.com/app/apikey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Gemini Interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Artificial intelligence (AI) is a broad field encompassing the theory and development of computer systems able to perform tasks that normally require human intelligence.  This includes tasks like:\n",
       "\n",
       "* **Learning:** Acquiring information and rules for using the information.\n",
       "* **Reasoning:** Using rules to reach approximate or definite conclusions.\n",
       "* **Problem-solving:** Finding solutions to complex situations.\n",
       "* **Perception:** Interpreting sensory information like images, sound, and text.\n",
       "* **Language understanding:**  Processing"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Simple Gemini call\n",
    "response = llm.invoke([HumanMessage(content=\"What is artificial intelligence?\")])\n",
    "display(Markdown(response.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Understanding System Prompts with Gemini\n",
    "\n",
    "System prompts define the AI's behavior and personality. Gemini handles system messages effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example with system prompt\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful coding assistant specializing in Python. Always provide clear, concise explanations with practical code examples.\"),\n",
    "    HumanMessage(content=\"Explain what a Python dictionary is\")\n",
    "]\n",
    "\n",
    "response = llm.invoke(messages)\n",
    "display(Markdown(response.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Create different personas with system prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different system prompts with Gemini\n",
    "personas = {\n",
    "    \"teacher\": \"You are a patient university professor explaining complex concepts to undergraduate students. Use analogies and examples.\",\n",
    "    \"expert\": \"You are a senior AI researcher providing detailed technical analysis with academic rigor.\",\n",
    "    \"creative\": \"You are an innovative storyteller who explains concepts through engaging narratives and metaphors.\"\n",
    "}\n",
    "\n",
    "question = \"What is deep learning?\"\n",
    "\n",
    "for persona, system_prompt in personas.items():\n",
    "    print(f\"\\n=== {persona.upper()} RESPONSE ===\")\n",
    "    messages = [\n",
    "        SystemMessage(content=system_prompt),\n",
    "        HumanMessage(content=question)\n",
    "    ]\n",
    "    response = llm.invoke(messages)\n",
    "    display(Markdown(response.content[:250] + \"...\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prompting Techniques with Gemini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Zero-shot Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero-shot: Direct question without examples\n",
    "prompt = \"Classify the sentiment of this text as Positive, Negative, or Neutral: 'I absolutely love this new feature!'\"\n",
    "response = llm.invoke([HumanMessage(content=prompt)])\n",
    "display(Markdown(f\"Zero-shot result: {response.content}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Few-shot Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few-shot: Provide examples for Gemini\n",
    "few_shot_prompt = \"\"\"\n",
    "Classify the sentiment of the following texts as Positive, Negative, or Neutral:\n",
    "\n",
    "Text: \"This application is fantastic and user-friendly!\"\n",
    "Sentiment: Positive\n",
    "\n",
    "Text: \"The service was terrible and slow.\"\n",
    "Sentiment: Negative\n",
    "\n",
    "Text: \"The weather is normal today.\"\n",
    "Sentiment: Neutral\n",
    "\n",
    "Text: \"I absolutely love this new feature!\"\n",
    "Sentiment:\n",
    "\"\"\"\n",
    "\n",
    "response = llm.invoke([HumanMessage(content=few_shot_prompt)])\n",
    "display(Markdown(f\"Few-shot result: {response.content}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Chain-of-Thought Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain-of-thought: Ask Gemini for step-by-step reasoning\n",
    "cot_prompt = \"\"\"\n",
    "Solve this step by step:\n",
    "\n",
    "A coffee shop has 20 tables. Each table can seat 3 people. \n",
    "If the coffee shop is 75% full, how many people are currently seated?\n",
    "\n",
    "Think through this step by step and show your work:\n",
    "\"\"\"\n",
    "\n",
    "response = llm.invoke([HumanMessage(content=cot_prompt)])\n",
    "display(Markdown(response.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increase token limit for more detailed responses\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=500\n",
    ")\n",
    "\n",
    "response = llm.invoke([HumanMessage(content=cot_prompt)])\n",
    "display(Markdown(response.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Introduction to LangGraph with Gemini\n",
    "\n",
    "LangGraph helps build stateful, multi-step workflows with Gemini as the underlying LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define state for our graph\n",
    "class GraphState(TypedDict):\n",
    "    messages: List[str]\n",
    "    current_step: str\n",
    "    result: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Simple Linear Workflow with Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_text(state: GraphState) -> GraphState:\n",
    "    \"\"\"Analyze the input text using Gemini\"\"\"\n",
    "    text = state[\"messages\"][-1]\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Analyze the following text and provide a structured analysis with:\n",
    "    1. Main topic\n",
    "    2. Sentiment analysis\n",
    "    3. Key themes and concepts\n",
    "    4. Writing style assessment\n",
    "    \n",
    "    Text: {text}\n",
    "    \"\"\"\n",
    "    \n",
    "    response = llm.invoke([HumanMessage(content=prompt)])\n",
    "    \n",
    "    state[\"current_step\"] = \"analysis\"\n",
    "    state[\"result\"] = response.content\n",
    "    \n",
    "    return state\n",
    "\n",
    "def summarize_analysis(state: GraphState) -> GraphState:\n",
    "    \"\"\"Create a summary of the analysis using Gemini\"\"\"\n",
    "    analysis = state[\"result\"]\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Create a concise executive summary of this analysis in 2-3 sentences.\n",
    "    Focus on the most important insights:\n",
    "    \n",
    "    {analysis}\n",
    "    \"\"\"\n",
    "    \n",
    "    response = llm.invoke([HumanMessage(content=prompt)])\n",
    "    \n",
    "    state[\"current_step\"] = \"summary\"\n",
    "    state[\"result\"] = response.content\n",
    "    \n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the graph with Gemini\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"analyze\", analyze_text)\n",
    "workflow.add_node(\"summarize\", summarize_analysis)\n",
    "\n",
    "# Add edges\n",
    "workflow.add_edge(\"analyze\", \"summarize\")\n",
    "\n",
    "# Set entry point\n",
    "workflow.set_entry_point(\"analyze\")\n",
    "workflow.set_finish_point(\"summarize\")\n",
    "\n",
    "# Compile the graph\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the workflow\n",
    "from IPython.display import Image\n",
    "\n",
    "Image(app.get_graph().draw_mermaid_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the workflow with Gemini\n",
    "initial_state = {\n",
    "    \"messages\": [\"Google's Gemini represents a significant breakthrough in multimodal AI capabilities. This advanced language model can process and understand text, images, and code simultaneously, opening new possibilities for creative problem-solving and innovative applications across various industries.\"],\n",
    "    \"current_step\": \"start\",\n",
    "    \"result\": \"\"\n",
    "}\n",
    "\n",
    "result = app.invoke(initial_state)\n",
    "print(\"Final Summary:\")\n",
    "display(Markdown(result[\"result\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Building a Conditional Workflow with Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_query(state: GraphState) -> GraphState:\n",
    "    \"\"\"Classify the type of query using Gemini\"\"\"\n",
    "    query = state[\"messages\"][-1]\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Classify this query into exactly one of these categories:\n",
    "    - technical: Programming, coding, software development, or technical questions\n",
    "    - creative: Creative writing, brainstorming, artistic requests, or storytelling\n",
    "    - factual: Factual questions, information requests, or knowledge queries\n",
    "    \n",
    "    Query: {query}\n",
    "    \n",
    "    Respond with only the category name (technical, creative, or factual).\n",
    "    \"\"\"\n",
    "    \n",
    "    response = llm.invoke([HumanMessage(content=prompt)])\n",
    "    classification = response.content.strip().lower()\n",
    "    \n",
    "    state[\"current_step\"] = \"classified\"\n",
    "    state[\"result\"] = classification\n",
    "    \n",
    "    return state\n",
    "\n",
    "def handle_technical(state: GraphState) -> GraphState:\n",
    "    \"\"\"Handle technical queries with Gemini\"\"\"\n",
    "    query = state[\"messages\"][-1]\n",
    "    \n",
    "    system_prompt = \"You are a senior software engineer with expertise in multiple programming languages. Provide detailed technical explanations with practical code examples and best practices.\"\n",
    "    \n",
    "    response = llm.invoke([\n",
    "        SystemMessage(content=system_prompt),\n",
    "        HumanMessage(content=query)\n",
    "    ])\n",
    "    \n",
    "    state[\"current_step\"] = \"technical_response\"\n",
    "    state[\"result\"] = response.content\n",
    "    \n",
    "    return state\n",
    "\n",
    "def handle_creative(state: GraphState) -> GraphState:\n",
    "    \"\"\"Handle creative queries with Gemini\"\"\"\n",
    "    query = state[\"messages\"][-1]\n",
    "    \n",
    "    system_prompt = \"You are a creative writing assistant and brainstorming expert. Be imaginative, inspiring, and help users explore innovative ideas with vivid descriptions and engaging narratives.\"\n",
    "    \n",
    "    response = llm.invoke([\n",
    "        SystemMessage(content=system_prompt),\n",
    "        HumanMessage(content=query)\n",
    "    ])\n",
    "    \n",
    "    state[\"current_step\"] = \"creative_response\"\n",
    "    state[\"result\"] = response.content\n",
    "    \n",
    "    return state\n",
    "\n",
    "def handle_factual(state: GraphState) -> GraphState:\n",
    "    \"\"\"Handle factual queries with Gemini\"\"\"\n",
    "    query = state[\"messages\"][-1]\n",
    "    \n",
    "    system_prompt = \"You are a knowledgeable research assistant. Provide accurate, well-structured information with relevant context and reliable details. Always strive for clarity and completeness.\"\n",
    "    \n",
    "    response = llm.invoke([\n",
    "        SystemMessage(content=system_prompt),\n",
    "        HumanMessage(content=query)\n",
    "    ])\n",
    "    \n",
    "    state[\"current_step\"] = \"factual_response\"\n",
    "    state[\"result\"] = response.content\n",
    "    \n",
    "    return state\n",
    "\n",
    "def route_query(state: GraphState) -> str:\n",
    "    \"\"\"Route to appropriate handler based on Gemini's classification\"\"\"\n",
    "    classification = state[\"result\"]\n",
    "    \n",
    "    if \"technical\" in classification:\n",
    "        return \"technical\"\n",
    "    elif \"creative\" in classification:\n",
    "        return \"creative\"\n",
    "    else:\n",
    "        return \"factual\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build conditional workflow with Gemini\n",
    "conditional_workflow = StateGraph(GraphState)\n",
    "\n",
    "# Add nodes\n",
    "conditional_workflow.add_node(\"classify\", classify_query)\n",
    "conditional_workflow.add_node(\"technical\", handle_technical)\n",
    "conditional_workflow.add_node(\"creative\", handle_creative)\n",
    "conditional_workflow.add_node(\"factual\", handle_factual)\n",
    "\n",
    "# Add conditional edges\n",
    "conditional_workflow.add_conditional_edges(\n",
    "    \"classify\",\n",
    "    route_query,\n",
    "    {\n",
    "        \"technical\": \"technical\",\n",
    "        \"creative\": \"creative\",\n",
    "        \"factual\": \"factual\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Set entry and finish points\n",
    "conditional_workflow.set_entry_point(\"classify\")\n",
    "conditional_workflow.set_finish_point(\"technical\")\n",
    "conditional_workflow.set_finish_point(\"creative\")\n",
    "conditional_workflow.set_finish_point(\"factual\")\n",
    "\n",
    "# Compile\n",
    "conditional_app = conditional_workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(conditional_app.get_graph().draw_mermaid_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the conditional workflow with Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with different query types using Gemini\n",
    "test_queries = [\n",
    "    \"How do I implement a recursive function to calculate Fibonacci numbers in Python?\",\n",
    "    \"Write a creative short story about an AI that discovers it can paint emotions\",\n",
    "    \"What are the main differences between supervised and unsupervised machine learning?\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\n=== QUERY: {query} ===\")\n",
    "    \n",
    "    state = {\n",
    "        \"messages\": [query],\n",
    "        \"current_step\": \"start\",\n",
    "        \"result\": \"\"\n",
    "    }\n",
    "    \n",
    "    result = conditional_app.invoke(state)\n",
    "    print(f\"Classification: {result['current_step']}\")\n",
    "    display(Markdown(f\"Response: {result['result'][:300]}...\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Exploring Gemini-Specific Features\n",
    "\n",
    "Gemini offers unique capabilities that can enhance your LangGraph workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Using different Gemini models\n",
    "gemini_pro = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-pro\",\n",
    "    temperature=0.3,\n",
    "    max_tokens=200\n",
    ")\n",
    "\n",
    "# Compare responses between models\n",
    "complex_query = \"Explain the concept of quantum entanglement and its implications for quantum computing\"\n",
    "\n",
    "print(\"=== Gemini Flash Response ===\")\n",
    "flash_response = llm.invoke([HumanMessage(content=complex_query)])\n",
    "display(Markdown(flash_response.content[:300] + \"...\"))\n",
    "\n",
    "print(\"\\n=== Gemini Pro Response ===\")\n",
    "pro_response = gemini_pro.invoke([HumanMessage(content=complex_query)])\n",
    "display(Markdown(pro_response.content[:300] + \"...\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nakara-skybound-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
