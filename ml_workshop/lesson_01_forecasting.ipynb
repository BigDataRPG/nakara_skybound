{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regional Load Forecasting Workshop\n",
    "## ‡∏Å‡∏≤‡∏£‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏ü‡πâ‡∏≤‡∏£‡∏≤‡∏¢‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà\n",
    "\n",
    "### Workshop Overview\n",
    "This workshop covers end-to-end machine learning for electrical load forecasting, including:\n",
    "- Data exploration and visualization\n",
    "- Feature engineering\n",
    "- Data imputation and preprocessing\n",
    "- Model training and validation\n",
    "- Performance evaluation\n",
    "- Inference with new data\n",
    "\n",
    "### Learning Objectives\n",
    "- Understand electrical load patterns and seasonality\n",
    "- Apply time series analysis techniques\n",
    "- Build predictive models for load forecasting\n",
    "- Evaluate model performance with appropriate metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning libraries\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "\n",
    "# Time series libraries\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate Synthetic Electrical Load Data\n",
    "Since we don't have real data, we'll create realistic synthetic data that mimics actual electrical load patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {},
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_electrical_load_data(start_date='2022-01-01', end_date='2024-01-01', freq='H'):\n",
    "    \"\"\"\n",
    "    Generate synthetic electrical load data with realistic patterns\n",
    "    \"\"\"\n",
    "    # Create date range\n",
    "    dates = pd.date_range(start=start_date, end=end_date, freq=freq)\n",
    "    n_samples = len(dates)\n",
    "    \n",
    "    # Base load (MW)\n",
    "    base_load = 1000\n",
    "    \n",
    "    # Seasonal patterns\n",
    "    # Annual seasonality (summer peaks due to AC usage)\n",
    "    annual_pattern = 200 * np.sin(2 * np.pi * dates.dayofyear / 365.25 - np.pi/2)\n",
    "    \n",
    "    # Weekly seasonality (weekday vs weekend)\n",
    "    weekly_pattern = 150 * (dates.weekday < 5).astype(int)  # Higher on weekdays\n",
    "    \n",
    "    # Daily seasonality (peak hours)\n",
    "    daily_pattern = 300 * np.sin(2 * np.pi * dates.hour / 24 - np.pi/2) + \\\n",
    "                   200 * np.sin(4 * np.pi * dates.hour / 24)  # Two peaks per day\n",
    "    \n",
    "    # Weather influence (temperature proxy)\n",
    "    temp_base = 25 + 10 * np.sin(2 * np.pi * dates.dayofyear / 365.25)\n",
    "    temp_variation = np.random.normal(0, 5, n_samples)\n",
    "    temperature = temp_base + temp_variation\n",
    "    \n",
    "    # Load increases with extreme temperatures (heating/cooling)\n",
    "    temp_load = 10 * np.abs(temperature - 22)  # Comfortable temperature is 22¬∞C\n",
    "    \n",
    "    # Random noise and special events\n",
    "    noise = np.random.normal(0, 50, n_samples)\n",
    "    \n",
    "    # Calculate total load\n",
    "    total_load = base_load + annual_pattern + weekly_pattern + daily_pattern + temp_load + noise\n",
    "    \n",
    "    # Ensure non-negative values\n",
    "    total_load = np.maximum(total_load, 100)\n",
    "    \n",
    "    # Create humidity data\n",
    "    humidity = 60 + 20 * np.sin(2 * np.pi * dates.dayofyear / 365.25) + np.random.normal(0, 10, n_samples)\n",
    "    humidity = np.clip(humidity, 20, 95)\n",
    "    \n",
    "    # Create wind speed data\n",
    "    wind_speed = 8 + 5 * np.sin(2 * np.pi * dates.dayofyear / 365.25) + np.random.exponential(2, n_samples)\n",
    "    wind_speed = np.clip(wind_speed, 0, 30)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'timestamp': dates,\n",
    "        'load_mw': total_load,\n",
    "        'temperature': temperature,\n",
    "        'humidity': humidity,\n",
    "        'wind_speed': wind_speed,\n",
    "        'region': np.random.choice(['North', 'South', 'Central', 'Northeast'], n_samples)\n",
    "    })\n",
    "    \n",
    "    # Add some missing values to simulate real-world data\n",
    "    missing_indices = np.random.choice(df.index, size=int(0.02 * len(df)), replace=False)\n",
    "    df.loc[missing_indices, 'temperature'] = np.nan\n",
    "    \n",
    "    missing_indices = np.random.choice(df.index, size=int(0.01 * len(df)), replace=False)\n",
    "    df.loc[missing_indices, 'humidity'] = np.nan\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Generate the dataset\n",
    "df = generate_electrical_load_data()\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Date range: {df.timestamp.min()} to {df.timestamp.max()}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {},
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Basic data information\n",
    "print(\"Dataset Info:\")\n",
    "print(df.info())\n",
    "print(\"\\nMissing values:\")\n",
    "print(df.isnull().sum())\n",
    "print(\"\\nBasic statistics:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Time series visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Load over time\n",
    "axes[0,0].plot(df.timestamp, df.load_mw, alpha=0.7)\n",
    "axes[0,0].set_title('Electrical Load Over Time')\n",
    "axes[0,0].set_ylabel('Load (MW)')\n",
    "axes[0,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Temperature over time\n",
    "axes[0,1].plot(df.timestamp, df.temperature, alpha=0.7, color='red')\n",
    "axes[0,1].set_title('Temperature Over Time')\n",
    "axes[0,1].set_ylabel('Temperature (¬∞C)')\n",
    "axes[0,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Load distribution\n",
    "axes[1,0].hist(df.load_mw, bins=50, alpha=0.7, edgecolor='black')\n",
    "axes[1,0].set_title('Load Distribution')\n",
    "axes[1,0].set_xlabel('Load (MW)')\n",
    "axes[1,0].set_ylabel('Frequency')\n",
    "\n",
    "# Load vs Temperature scatter\n",
    "axes[1,1].scatter(df.temperature, df.load_mw, alpha=0.5)\n",
    "axes[1,1].set_title('Load vs Temperature')\n",
    "axes[1,1].set_xlabel('Temperature (¬∞C)')\n",
    "axes[1,1].set_ylabel('Load (MW)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Seasonal decomposition\n",
    "df_sample = df.set_index('timestamp').resample('D').mean()  # Daily averages for decomposition\n",
    "\n",
    "# Perform seasonal decomposition\n",
    "decomposition = seasonal_decompose(df_sample['load_mw'], model='additive', period=365)\n",
    "\n",
    "# Plot decomposition\n",
    "fig, axes = plt.subplots(4, 1, figsize=(15, 12))\n",
    "\n",
    "decomposition.observed.plot(ax=axes[0], title='Original')\n",
    "decomposition.trend.plot(ax=axes[1], title='Trend')\n",
    "decomposition.seasonal.plot(ax=axes[2], title='Seasonal')\n",
    "decomposition.resid.plot(ax=axes[3], title='Residual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering\n",
    "Create additional features that can help improve load forecasting accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "def create_time_features(df):\n",
    "    \"\"\"\n",
    "    Create time-based features for load forecasting\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Extract time components\n",
    "    df['hour'] = df.timestamp.dt.hour\n",
    "    df['day_of_week'] = df.timestamp.dt.dayofweek\n",
    "    df['day_of_year'] = df.timestamp.dt.dayofyear\n",
    "    df['month'] = df.timestamp.dt.month\n",
    "    df['quarter'] = df.timestamp.dt.quarter\n",
    "    df['year'] = df.timestamp.dt.year\n",
    "    \n",
    "    # Cyclical encoding for periodic features\n",
    "    df['hour_sin'] = np.sin(2 * np.pi * df.hour / 24)\n",
    "    df['hour_cos'] = np.cos(2 * np.pi * df.hour / 24)\n",
    "    \n",
    "    df['day_sin'] = np.sin(2 * np.pi * df.day_of_week / 7)\n",
    "    df['day_cos'] = np.cos(2 * np.pi * df.day_of_week / 7)\n",
    "    \n",
    "    df['month_sin'] = np.sin(2 * np.pi * df.month / 12)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df.month / 12)\n",
    "    \n",
    "    # Binary features\n",
    "    df['is_weekend'] = (df.day_of_week >= 5).astype(int)\n",
    "    df['is_summer'] = df.month.isin([6, 7, 8]).astype(int)\n",
    "    df['is_winter'] = df.month.isin([12, 1, 2]).astype(int)\n",
    "    \n",
    "    # Peak hours (typically 9-11 AM and 6-9 PM)\n",
    "    df['is_peak_morning'] = df.hour.isin([9, 10, 11]).astype(int)\n",
    "    df['is_peak_evening'] = df.hour.isin([18, 19, 20, 21]).astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_lag_features(df, target_col='load_mw', lags=[1, 2, 3, 24, 48, 168]):\n",
    "    \"\"\"\n",
    "    Create lag features for time series forecasting\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    for lag in lags:\n",
    "        df[f'{target_col}_lag_{lag}'] = df[target_col].shift(lag)\n",
    "    \n",
    "    # Rolling statistics\n",
    "    df[f'{target_col}_rolling_mean_24'] = df[target_col].rolling(window=24, min_periods=1).mean()\n",
    "    df[f'{target_col}_rolling_std_24'] = df[target_col].rolling(window=24, min_periods=1).std()\n",
    "    df[f'{target_col}_rolling_mean_168'] = df[target_col].rolling(window=168, min_periods=1).mean()\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_weather_features(df):\n",
    "    \"\"\"\n",
    "    Create weather-derived features\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Temperature-based features\n",
    "    df['temp_squared'] = df.temperature ** 2\n",
    "    df['temp_deviation_from_comfort'] = np.abs(df.temperature - 22)  # 22¬∞C as comfort zone\n",
    "    \n",
    "    # Heat index approximation\n",
    "    df['heat_index'] = df.temperature + 0.5 * df.humidity\n",
    "    \n",
    "    # Weather categories\n",
    "    df['temp_category'] = pd.cut(df.temperature, bins=[-float('inf'), 10, 20, 30, float('inf')], \n",
    "                                labels=['Cold', 'Cool', 'Warm', 'Hot'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply feature engineering\n",
    "df_features = create_time_features(df)\n",
    "df_features = create_lag_features(df_features)\n",
    "df_features = create_weather_features(df_features)\n",
    "\n",
    "print(f\"Original features: {df.shape[1]}\")\n",
    "print(f\"After feature engineering: {df_features.shape[1]}\")\n",
    "print(\"\\nNew features created:\")\n",
    "new_features = set(df_features.columns) - set(df.columns)\n",
    "print(list(new_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Imputation and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Check missing values after feature engineering\n",
    "print(\"Missing values after feature engineering:\")\n",
    "missing_summary = df_features.isnull().sum()\n",
    "print(missing_summary[missing_summary > 0])\n",
    "\n",
    "# Separate numerical and categorical columns\n",
    "numerical_cols = df_features.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = ['region', 'temp_category']\n",
    "\n",
    "print(f\"\\nNumerical columns: {len(numerical_cols)}\")\n",
    "print(f\"Categorical columns: {len(categorical_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Imputation strategies\n",
    "def impute_missing_values(df, numerical_cols, categorical_cols):\n",
    "    \"\"\"\n",
    "    Apply different imputation strategies for missing values\n",
    "    \"\"\"\n",
    "    df_imputed = df.copy()\n",
    "    \n",
    "    # For numerical columns - use KNN imputation for weather features\n",
    "    weather_cols = ['temperature', 'humidity', 'wind_speed']\n",
    "    weather_cols_present = [col for col in weather_cols if col in df_imputed.columns]\n",
    "    \n",
    "    if weather_cols_present:\n",
    "        knn_imputer = KNNImputer(n_neighbors=5)\n",
    "        df_imputed[weather_cols_present] = knn_imputer.fit_transform(df_imputed[weather_cols_present])\n",
    "    \n",
    "    # For other numerical columns with missing values - use median\n",
    "    other_numerical = [col for col in numerical_cols if col not in weather_cols_present]\n",
    "    \n",
    "    for col in other_numerical:\n",
    "        if df_imputed[col].isnull().sum() > 0:\n",
    "            median_imputer = SimpleImputer(strategy='median')\n",
    "            df_imputed[[col]] = median_imputer.fit_transform(df_imputed[[col]])\n",
    "    \n",
    "    # For categorical columns - use mode\n",
    "    for col in categorical_cols:\n",
    "        if col in df_imputed.columns and df_imputed[col].isnull().sum() > 0:\n",
    "            mode_imputer = SimpleImputer(strategy='most_frequent')\n",
    "            df_imputed[[col]] = mode_imputer.fit_transform(df_imputed[[col]])\n",
    "    \n",
    "    return df_imputed\n",
    "\n",
    "# Apply imputation\n",
    "df_clean = impute_missing_values(df_features, numerical_cols, categorical_cols)\n",
    "\n",
    "# Verify no missing values remain\n",
    "print(\"Missing values after imputation:\")\n",
    "print(df_clean.isnull().sum().sum())\n",
    "\n",
    "# Recalculate weather features after imputation\n",
    "df_clean = create_weather_features(df_clean)\n",
    "\n",
    "print(\"Data imputation completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Encode categorical variables\n",
    "def encode_categorical_features(df, categorical_cols):\n",
    "    \"\"\"\n",
    "    Encode categorical features using appropriate methods\n",
    "    \"\"\"\n",
    "    df_encoded = df.copy()\n",
    "    \n",
    "    # One-hot encoding for region\n",
    "    if 'region' in df_encoded.columns:\n",
    "        region_dummies = pd.get_dummies(df_encoded['region'], prefix='region')\n",
    "        df_encoded = pd.concat([df_encoded, region_dummies], axis=1)\n",
    "        df_encoded.drop('region', axis=1, inplace=True)\n",
    "    \n",
    "    # Label encoding for ordinal categorical variables\n",
    "    if 'temp_category' in df_encoded.columns:\n",
    "        le = LabelEncoder()\n",
    "        df_encoded['temp_category_encoded'] = le.fit_transform(df_encoded['temp_category'].astype(str))\n",
    "        df_encoded.drop('temp_category', axis=1, inplace=True)\n",
    "    \n",
    "    return df_encoded\n",
    "\n",
    "# Apply encoding\n",
    "df_encoded = encode_categorical_features(df_clean, categorical_cols)\n",
    "\n",
    "print(f\"Shape after encoding: {df_encoded.shape}\")\n",
    "print(\"Data preprocessing completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Splitting and Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Prepare data for modeling\n",
    "# Remove timestamp and target variable from features\n",
    "feature_cols = [col for col in df_encoded.columns if col not in ['timestamp', 'load_mw']]\n",
    "target_col = 'load_mw'\n",
    "\n",
    "# Remove rows with NaN values (mainly from lag features)\n",
    "df_model = df_encoded.dropna().copy()\n",
    "\n",
    "X = df_model[feature_cols]\n",
    "y = df_model[target_col]\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Target vector shape: {y.shape}\")\n",
    "print(f\"\\nFeatures used: {len(feature_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Time series split for proper validation\n",
    "# Use the last 20% of data for testing\n",
    "split_index = int(0.8 * len(df_model))\n",
    "\n",
    "X_train = X.iloc[:split_index]\n",
    "X_test = X.iloc[split_index:]\n",
    "y_train = y.iloc[:split_index]\n",
    "y_test = y.iloc[split_index:]\n",
    "\n",
    "# Get corresponding timestamps for analysis\n",
    "train_timestamps = df_model['timestamp'].iloc[:split_index]\n",
    "test_timestamps = df_model['timestamp'].iloc[split_index:]\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "print(f\"Training period: {train_timestamps.min()} to {train_timestamps.max()}\")\n",
    "print(f\"Test period: {test_timestamps.min()} to {test_timestamps.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert back to DataFrames for easier handling\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=feature_cols, index=X_train.index)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=feature_cols, index=X_test.index)\n",
    "\n",
    "print(\"Feature scaling completed!\")\n",
    "print(f\"Feature means after scaling: {X_train_scaled.mean().round(3).head()}\")\n",
    "print(f\"Feature stds after scaling: {X_train_scaled.std().round(3).head()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Define evaluation metrics\n",
    "def evaluate_model(y_true, y_pred, model_name):\n",
    "    \"\"\"\n",
    "    Calculate and display evaluation metrics\n",
    "    \"\"\"\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "    \n",
    "    print(f\"\\n{model_name} Performance:\")\n",
    "    print(f\"MAE: {mae:.2f} MW\")\n",
    "    print(f\"RMSE: {rmse:.2f} MW\")\n",
    "    print(f\"R¬≤: {r2:.4f}\")\n",
    "    print(f\"MAPE: {mape:.2f}%\")\n",
    "    \n",
    "    return {'MAE': mae, 'RMSE': rmse, 'R2': r2, 'MAPE': mape}\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "# Train and evaluate models\n",
    "model_results = {}\n",
    "trained_models = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    # Train model\n",
    "    if name == 'Linear Regression':\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Evaluate model\n",
    "    results = evaluate_model(y_test, y_pred, name)\n",
    "    model_results[name] = results\n",
    "    trained_models[name] = (model, y_pred)\n",
    "\n",
    "print(\"\\nModel training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Cross-validation with time series split\n",
    "def time_series_cv(X, y, model, n_splits=5):\n",
    "    \"\"\"\n",
    "    Perform time series cross-validation\n",
    "    \"\"\"\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    cv_scores = []\n",
    "    \n",
    "    for train_idx, val_idx in tscv.split(X):\n",
    "        X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        \n",
    "        model.fit(X_tr, y_tr)\n",
    "        y_pred = model.predict(X_val)\n",
    "        \n",
    "        mae = mean_absolute_error(y_val, y_pred)\n",
    "        cv_scores.append(mae)\n",
    "    \n",
    "    return cv_scores\n",
    "\n",
    "# Perform cross-validation for Random Forest (best performing model)\n",
    "print(\"Performing time series cross-validation...\")\n",
    "rf_cv_scores = time_series_cv(X_train, y_train, RandomForestRegressor(n_estimators=100, random_state=42))\n",
    "\n",
    "print(f\"Cross-validation MAE scores: {[f'{score:.2f}' for score in rf_cv_scores]}\")\n",
    "print(f\"Mean CV MAE: {np.mean(rf_cv_scores):.2f} ¬± {np.std(rf_cv_scores):.2f} MW\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Evaluation and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Comparison of model performance\n",
    "results_df = pd.DataFrame(model_results).T\n",
    "print(\"Model Performance Comparison:\")\n",
    "print(results_df)\n",
    "\n",
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "metrics = ['MAE', 'RMSE', 'R2', 'MAPE']\n",
    "for i, metric in enumerate(metrics):\n",
    "    ax = axes[i//2, i%2]\n",
    "    results_df[metric].plot(kind='bar', ax=ax, color=['skyblue', 'lightgreen', 'lightcoral'])\n",
    "    ax.set_title(f'{metric} Comparison')\n",
    "    ax.set_ylabel(metric)\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Detailed visualization of predictions\n",
    "best_model_name = 'Random Forest'  # Based on typical performance\n",
    "best_model, best_predictions = trained_models[best_model_name]\n",
    "\n",
    "# Time series plot of predictions vs actual\n",
    "fig, axes = plt.subplots(3, 1, figsize=(15, 12))\n",
    "\n",
    "# Full test period\n",
    "axes[0].plot(test_timestamps, y_test.values, label='Actual', alpha=0.8)\n",
    "axes[0].plot(test_timestamps, best_predictions, label='Predicted', alpha=0.8)\n",
    "axes[0].set_title(f'{best_model_name} Predictions vs Actual - Full Test Period')\n",
    "axes[0].set_ylabel('Load (MW)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Zoomed view (first week)\n",
    "week_mask = test_timestamps <= test_timestamps.min() + timedelta(days=7)\n",
    "axes[1].plot(test_timestamps[week_mask], y_test[week_mask], label='Actual', marker='o', markersize=2)\n",
    "axes[1].plot(test_timestamps[week_mask], best_predictions[week_mask], label='Predicted', marker='s', markersize=2)\n",
    "axes[1].set_title('First Week - Detailed View')\n",
    "axes[1].set_ylabel('Load (MW)')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Residuals\n",
    "residuals = y_test.values - best_predictions\n",
    "axes[2].plot(test_timestamps, residuals, alpha=0.7, color='red')\n",
    "axes[2].axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "axes[2].set_title('Prediction Residuals')\n",
    "axes[2].set_ylabel('Residual (MW)')\n",
    "axes[2].set_xlabel('Time')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Scatter plot and residual analysis\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Predicted vs Actual scatter plot\n",
    "axes[0,0].scatter(y_test, best_predictions, alpha=0.6)\n",
    "axes[0,0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "axes[0,0].set_xlabel('Actual Load (MW)')\n",
    "axes[0,0].set_ylabel('Predicted Load (MW)')\n",
    "axes[0,0].set_title('Predicted vs Actual')\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Residuals distribution\n",
    "axes[0,1].hist(residuals, bins=50, alpha=0.7, edgecolor='black')\n",
    "axes[0,1].set_xlabel('Residuals (MW)')\n",
    "axes[0,1].set_ylabel('Frequency')\n",
    "axes[0,1].set_title('Residuals Distribution')\n",
    "axes[0,1].axvline(x=0, color='red', linestyle='--')\n",
    "\n",
    "# QQ plot for residuals normality\n",
    "from scipy import stats\n",
    "stats.probplot(residuals, dist=\"norm\", plot=axes[1,0])\n",
    "axes[1,0].set_title('Q-Q Plot of Residuals')\n",
    "\n",
    "# Residuals vs predicted values\n",
    "axes[1,1].scatter(best_predictions, residuals, alpha=0.6)\n",
    "axes[1,1].axhline(y=0, color='red', linestyle='--')\n",
    "axes[1,1].set_xlabel('Predicted Load (MW)')\n",
    "axes[1,1].set_ylabel('Residuals (MW)')\n",
    "axes[1,1].set_title('Residuals vs Predicted')\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Feature importance for Random Forest\n",
    "if best_model_name == 'Random Forest':\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': feature_cols,\n",
    "        'importance': best_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    # Plot top 20 most important features\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    top_features = feature_importance.head(20)\n",
    "    \n",
    "    plt.barh(range(len(top_features)), top_features['importance'], \n",
    "             color=plt.cm.viridis(np.linspace(0, 1, len(top_features))))\n",
    "    plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "    plt.xlabel('Feature Importance')\n",
    "    plt.title('Top 20 Most Important Features (Random Forest)')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Top 10 Most Important Features:\")\n",
    "    print(feature_importance.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Inference with New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Create a function for making predictions on new data\n",
    "def predict_load(model, scaler, new_data, model_name, feature_cols):\n",
    "    \"\"\"\n",
    "    Make load predictions on new data\n",
    "    \"\"\"\n",
    "    # Ensure new_data has all required features\n",
    "    for col in feature_cols:\n",
    "        if col not in new_data.columns:\n",
    "            print(f\"Warning: Missing feature {col}, setting to 0\")\n",
    "            new_data[col] = 0\n",
    "    \n",
    "    # Select and order features correctly\n",
    "    X_new = new_data[feature_cols]\n",
    "    \n",
    "    # Scale if needed (for Linear Regression)\n",
    "    if model_name == 'Linear Regression':\n",
    "        X_new_scaled = scaler.transform(X_new)\n",
    "        predictions = model.predict(X_new_scaled)\n",
    "    else:\n",
    "        predictions = model.predict(X_new)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# Generate future data for demonstration\n",
    "def generate_future_data(start_date, periods=168):  # 1 week of hourly data\n",
    "    \"\"\"\n",
    "    Generate future data with weather forecasts for demonstration\n",
    "    \"\"\"\n",
    "    future_dates = pd.date_range(start=start_date, periods=periods, freq='H')\n",
    "    \n",
    "    # Simulate weather forecast (with some uncertainty)\n",
    "    base_temp = 25 + 10 * np.sin(2 * np.pi * future_dates.dayofyear / 365.25)\n",
    "    temperature = base_temp + np.random.normal(0, 2, periods)  # Less variation in forecast\n",
    "    \n",
    "    humidity = 60 + 20 * np.sin(2 * np.pi * future_dates.dayofyear / 365.25) + np.random.normal(0, 5, periods)\n",
    "    humidity = np.clip(humidity, 20, 95)\n",
    "    \n",
    "    wind_speed = 8 + 5 * np.sin(2 * np.pi * future_dates.dayofyear / 365.25) + np.random.exponential(1.5, periods)\n",
    "    wind_speed = np.clip(wind_speed, 0, 25)\n",
    "    \n",
    "    future_df = pd.DataFrame({\n",
    "        'timestamp': future_dates,\n",
    "        'temperature': temperature,\n",
    "        'humidity': humidity,\n",
    "        'wind_speed': wind_speed,\n",
    "        'region': np.random.choice(['North', 'South', 'Central', 'Northeast'], periods)\n",
    "    })\n",
    "    \n",
    "    return future_df\n",
    "\n",
    "# Generate future scenario\n",
    "future_start = test_timestamps.max() + timedelta(hours=1)\n",
    "future_data = generate_future_data(future_start)\n",
    "\n",
    "print(f\"Generated future data for prediction:\")\n",
    "print(f\"Period: {future_data.timestamp.min()} to {future_data.timestamp.max()}\")\n",
    "print(f\"Shape: {future_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Prepare future data for prediction\n",
    "def prepare_future_data_for_prediction(future_data, last_known_load_values):\n",
    "    \"\"\"\n",
    "    Prepare future data by adding all necessary features\n",
    "    Note: In practice, lag features would need special handling\n",
    "    \"\"\"\n",
    "    # Add time features\n",
    "    future_processed = create_time_features(future_data)\n",
    "    \n",
    "    # Add weather features\n",
    "    future_processed = create_weather_features(future_processed)\n",
    "    \n",
    "    # Encode categorical features\n",
    "    future_processed = encode_categorical_features(future_processed, ['region', 'temp_category'])\n",
    "    \n",
    "    # For lag features, we'll use the last known values as approximation\n",
    "    # In practice, you'd need a more sophisticated approach for multi-step prediction\n",
    "    lag_features = [col for col in feature_cols if 'lag' in col or 'rolling' in col]\n",
    "    \n",
    "    for feature in lag_features:\n",
    "        if 'lag_1' in feature:\n",
    "            future_processed[feature] = last_known_load_values[-1]\n",
    "        elif 'lag_2' in feature:\n",
    "            future_processed[feature] = last_known_load_values[-2] if len(last_known_load_values) > 1 else last_known_load_values[-1]\n",
    "        elif 'lag_3' in feature:\n",
    "            future_processed[feature] = last_known_load_values[-3] if len(last_known_load_values) > 2 else last_known_load_values[-1]\n",
    "        elif 'lag_24' in feature:\n",
    "            future_processed[feature] = np.mean(last_known_load_values[-24:]) if len(last_known_load_values) >= 24 else last_known_load_values[-1]\n",
    "        elif 'rolling_mean_24' in feature:\n",
    "            future_processed[feature] = np.mean(last_known_load_values[-24:]) if len(last_known_load_values) >= 24 else np.mean(last_known_load_values)\n",
    "        elif 'rolling_std_24' in feature:\n",
    "            future_processed[feature] = np.std(last_known_load_values[-24:]) if len(last_known_load_values) >= 24 else np.std(last_known_load_values) if len(last_known_load_values) > 1 else 0\n",
    "        elif 'rolling_mean_168' in feature:\n",
    "            future_processed[feature] = np.mean(last_known_load_values[-168:]) if len(last_known_load_values) >= 168 else np.mean(last_known_load_values)\n",
    "        else:\n",
    "            future_processed[feature] = np.mean(last_known_load_values[-48:]) if len(last_known_load_values) >= 48 else np.mean(last_known_load_values)\n",
    "    \n",
    "    return future_processed\n",
    "\n",
    "# Get last known load values from test set\n",
    "last_known_loads = y_test.values[-168:]  # Last week of known data\n",
    "\n",
    "# Prepare future data\n",
    "future_prepared = prepare_future_data_for_prediction(future_data, last_known_loads)\n",
    "\n",
    "# Make predictions\n",
    "future_predictions = predict_load(best_model, scaler, future_prepared, best_model_name, feature_cols)\n",
    "\n",
    "print(f\"Generated {len(future_predictions)} load predictions\")\n",
    "print(f\"Predicted load range: {future_predictions.min():.2f} - {future_predictions.max():.2f} MW\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize future predictions\n",
    "fig, axes = plt.subplots(3, 1, figsize=(15, 12))\n",
    "\n",
    "# Historical + Future predictions\n",
    "historical_period = test_timestamps[-168:]  # Last week of test data\n",
    "historical_actual = y_test.iloc[-168:].values\n",
    "historical_pred = best_predictions[-168:]\n",
    "\n",
    "# Plot historical data\n",
    "axes[0].plot(historical_period, historical_actual, label='Historical Actual', linewidth=2)\n",
    "axes[0].plot(historical_period, historical_pred, label='Historical Predicted', linewidth=2, alpha=0.8)\n",
    "\n",
    "# Plot future predictions\n",
    "axes[0].plot(future_data['timestamp'], future_predictions, label='Future Predictions', \n",
    "            linewidth=2, linestyle='--', color='red')\n",
    "axes[0].axvline(x=future_start, color='black', linestyle=':', alpha=0.7, label='Forecast Start')\n",
    "axes[0].set_title('Load Forecasting: Historical Performance and Future Predictions')\n",
    "axes[0].set_ylabel('Load (MW)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Future predictions only\n",
    "axes[1].plot(future_data['timestamp'], future_predictions, marker='o', markersize=3, linewidth=2)\n",
    "axes[1].set_title('Future Load Predictions (Next Week)')\n",
    "axes[1].set_ylabel('Load (MW)')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Weather context for future period\n",
    "ax2 = axes[2]\n",
    "ax3 = ax2.twinx()\n",
    "\n",
    "ax2.plot(future_data['timestamp'], future_data['temperature'], color='red', label='Temperature', linewidth=2)\n",
    "ax3.plot(future_data['timestamp'], future_predictions, color='blue', label='Predicted Load', linewidth=2)\n",
    "\n",
    "ax2.set_ylabel('Temperature (¬∞C)', color='red')\n",
    "ax3.set_ylabel('Load (MW)', color='blue')\n",
    "ax2.set_title('Temperature vs Predicted Load')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Add legends\n",
    "lines1, labels1 = ax2.get_legend_handles_labels()\n",
    "lines2, labels2 = ax3.get_legend_handles_labels()\n",
    "ax2.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary and Key Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Summary statistics and insights\n",
    "print(\"=\" * 60)\n",
    "print(\"REGIONAL LOAD FORECASTING WORKSHOP SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nüìä DATASET OVERVIEW:\")\n",
    "print(f\"‚Ä¢ Total samples: {len(df):,}\")\n",
    "print(f\"‚Ä¢ Time period: {df.timestamp.min().strftime('%Y-%m-%d')} to {df.timestamp.max().strftime('%Y-%m-%d')}\")\n",
    "print(f\"‚Ä¢ Features created: {len(feature_cols)}\")\n",
    "print(f\"‚Ä¢ Training samples: {len(X_train):,}\")\n",
    "print(f\"‚Ä¢ Test samples: {len(X_test):,}\")\n",
    "\n",
    "print(\"\\nüéØ MODEL PERFORMANCE:\")\n",
    "for model_name, metrics in model_results.items():\n",
    "    print(f\"‚Ä¢ {model_name}:\")\n",
    "    print(f\"  - MAE: {metrics['MAE']:.2f} MW\")\n",
    "    print(f\"  - RMSE: {metrics['RMSE']:.2f} MW\")\n",
    "    print(f\"  - R¬≤: {metrics['R2']:.4f}\")\n",
    "    print(f\"  - MAPE: {metrics['MAPE']:.2f}%\")\n",
    "\n",
    "print(f\"\\nüèÜ BEST MODEL: {best_model_name}\")\n",
    "best_results = model_results[best_model_name]\n",
    "print(f\"‚Ä¢ Accuracy: {best_results['R2']:.1%}\")\n",
    "print(f\"‚Ä¢ Average error: {best_results['MAE']:.2f} MW\")\n",
    "print(f\"‚Ä¢ Error percentage: {best_results['MAPE']:.2f}%\")\n",
    "\n",
    "print(\"\\nüîç KEY INSIGHTS:\")\n",
    "print(\"‚Ä¢ Load patterns show strong daily and weekly seasonality\")\n",
    "print(\"‚Ä¢ Temperature is a major driver of electricity demand\")\n",
    "print(\"‚Ä¢ Historical load values (lag features) are highly predictive\")\n",
    "print(\"‚Ä¢ Peak demand occurs during extreme weather conditions\")\n",
    "print(\"‚Ä¢ Weekdays generally have higher demand than weekends\")\n",
    "\n",
    "if best_model_name == 'Random Forest':\n",
    "    print(\"\\nüåü TOP PREDICTIVE FEATURES:\")\n",
    "    for i, (feature, importance) in enumerate(feature_importance.head(5).values):\n",
    "        print(f\"‚Ä¢ {i+1}. {feature}: {importance:.4f}\")\n",
    "\n",
    "print(\"\\nüöÄ FUTURE PREDICTIONS:\")\n",
    "print(f\"‚Ä¢ Forecast period: {future_data.timestamp.min().strftime('%Y-%m-%d %H:%M')} to {future_data.timestamp.max().strftime('%Y-%m-%d %H:%M')}\")\n",
    "print(f\"‚Ä¢ Predicted load range: {future_predictions.min():.0f} - {future_predictions.max():.0f} MW\")\n",
    "print(f\"‚Ä¢ Average predicted load: {future_predictions.mean():.0f} MW\")\n",
    "\n",
    "print(\"\\nüí° RECOMMENDATIONS:\")\n",
    "print(\"‚Ä¢ Monitor weather forecasts for load planning\")\n",
    "print(\"‚Ä¢ Consider regional differences in load patterns\")\n",
    "print(\"‚Ä¢ Update models regularly with new data\")\n",
    "print(\"‚Ä¢ Implement ensemble methods for improved accuracy\")\n",
    "print(\"‚Ä¢ Add external factors (holidays, economic indicators)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nakara-skybound-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
