{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regional Load Forecasting Workshop\n",
    "## การพยากรณ์โหลดไฟฟ้ารายพื้นที่\n",
    "\n",
    "### Workshop Overview\n",
    "This workshop covers end-to-end machine learning for electrical load forecasting, including:\n",
    "- Data exploration and visualization\n",
    "- Feature engineering\n",
    "- Data imputation and preprocessing\n",
    "- Model training and validation\n",
    "- Performance evaluation\n",
    "- Inference with new data\n",
    "\n",
    "### Learning Objectives\n",
    "- Understand electrical load patterns and seasonality\n",
    "- Apply time series analysis techniques\n",
    "- Build predictive models for load forecasting\n",
    "- Evaluate model performance with appropriate metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning libraries\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "\n",
    "# Time series libraries\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate Synthetic Electrical Load Data\n",
    "Since we don't have real data, we'll create realistic synthetic data that mimics actual electrical load patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {},
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_electrical_load_data(start_date='2022-01-01', end_date='2024-01-01', freq='H'):\n",
    "    \"\"\"\n",
    "    Generate synthetic electrical load data with realistic patterns\n",
    "    \"\"\"\n",
    "    # Create date range\n",
    "    dates = pd.date_range(start=start_date, end=end_date, freq=freq)\n",
    "    n_samples = len(dates)\n",
    "    \n",
    "    # Base load (MW)\n",
    "    base_load = 1000\n",
    "    \n",
    "    # Seasonal patterns\n",
    "    # Annual seasonality (summer peaks due to AC usage)\n",
    "    annual_pattern = 200 * np.sin(2 * np.pi * dates.dayofyear / 365.25 - np.pi/2)\n",
    "    \n",
    "    # Weekly seasonality (weekday vs weekend)\n",
    "    weekly_pattern = 150 * (dates.weekday < 5).astype(int)  # Higher on weekdays\n",
    "    \n",
    "    # Daily seasonality (peak hours)\n",
    "    daily_pattern = 300 * np.sin(2 * np.pi * dates.hour / 24 - np.pi/2) + \\\n",
    "                   200 * np.sin(4 * np.pi * dates.hour / 24)  # Two peaks per day\n",
    "    \n",
    "    # Weather influence (temperature proxy)\n",
    "    temp_base = 25 + 10 * np.sin(2 * np.pi * dates.dayofyear / 365.25)\n",
    "    temp_variation = np.random.normal(0, 5, n_samples)\n",
    "    temperature = temp_base + temp_variation\n",
    "    \n",
    "    # Load increases with extreme temperatures (heating/cooling)\n",
    "    temp_load = 10 * np.abs(temperature - 22)  # Comfortable temperature is 22°C\n",
    "    \n",
    "    # Random noise and special events\n",
    "    noise = np.random.normal(0, 50, n_samples)\n",
    "    \n",
    "    # Calculate total load\n",
    "    total_load = base_load + annual_pattern + weekly_pattern + daily_pattern + temp_load + noise\n",
    "    \n",
    "    # Ensure non-negative values\n",
    "    total_load = np.maximum(total_load, 100)\n",
    "    \n",
    "    # Create humidity data\n",
    "    humidity = 60 + 20 * np.sin(2 * np.pi * dates.dayofyear / 365.25) + np.random.normal(0, 10, n_samples)\n",
    "    humidity = np.clip(humidity, 20, 95)\n",
    "    \n",
    "    # Create wind speed data\n",
    "    wind_speed = 8 + 5 * np.sin(2 * np.pi * dates.dayofyear / 365.25) + np.random.exponential(2, n_samples)\n",
    "    wind_speed = np.clip(wind_speed, 0, 30)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'timestamp': dates,\n",
    "        'load_mw': total_load,\n",
    "        'temperature': temperature,\n",
    "        'humidity': humidity,\n",
    "        'wind_speed': wind_speed,\n",
    "        'region': np.random.choice(['North', 'South', 'Central', 'Northeast'], n_samples)\n",
    "    })\n",
    "    \n",
    "    # Add some missing values to simulate real-world data\n",
    "    missing_indices = np.random.choice(df.index, size=int(0.02 * len(df)), replace=False)\n",
    "    df.loc[missing_indices, 'temperature'] = np.nan\n",
    "    \n",
    "    missing_indices = np.random.choice(df.index, size=int(0.01 * len(df)), replace=False)\n",
    "    df.loc[missing_indices, 'humidity'] = np.nan\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Generate the dataset\n",
    "df = generate_electrical_load_data()\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Date range: {df.timestamp.min()} to {df.timestamp.max()}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {},
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Basic data information\n",
    "print(\"Dataset Info:\")\n",
    "print(df.info())\n",
    "print(\"\\nMissing values:\")\n",
    "print(df.isnull().sum())\n",
    "print(\"\\nBasic statistics:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Time series visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Load over time\n",
    "axes[0,0].plot(df.timestamp, df.load_mw, alpha=0.7)\n",
    "axes[0,0].set_title('Electrical Load Over Time')\n",
    "axes[0,0].set_ylabel('Load (MW)')\n",
    "axes[0,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Temperature over time\n",
    "axes[0,1].plot(df.timestamp, df.temperature, alpha=0.7, color='red')\n",
    "axes[0,1].set_title('Temperature Over Time')\n",
    "axes[0,1].set_ylabel('Temperature (°C)')\n",
    "axes[0,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Load distribution\n",
    "axes[1,0].hist(df.load_mw, bins=50, alpha=0.7, edgecolor='black')\n",
    "axes[1,0].set_title('Load Distribution')\n",
    "axes[1,0].set_xlabel('Load (MW)')\n",
    "axes[1,0].set_ylabel('Frequency')\n",
    "\n",
    "# Load vs Temperature scatter\n",
    "axes[1,1].scatter(df.temperature, df.load_mw, alpha=0.5)\n",
    "axes[1,1].set_title('Load vs Temperature')\n",
    "axes[1,1].set_xlabel('Temperature (°C)')\n",
    "axes[1,1].set_ylabel('Load (MW)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Seasonal decomposition\n",
    "df_sample = df.set_index('timestamp').resample('D').mean()  # Daily averages for decomposition\n",
    "\n",
    "# Perform seasonal decomposition\n",
    "decomposition = seasonal_decompose(df_sample['load_mw'], model='additive', period=365)\n",
    "\n",
    "# Plot decomposition\n",
    "fig, axes = plt.subplots(4, 1, figsize=(15, 12))\n",
    "\n",
    "decomposition.observed.plot(ax=axes[0], title='Original')\n",
    "decomposition.trend.plot(ax=axes[1], title='Trend')\n",
    "decomposition.seasonal.plot(ax=axes[2], title='Seasonal')\n",
    "decomposition.resid.plot(ax=axes[3], title='Residual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering\n",
    "Create additional features that can help improve load forecasting accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "def create_time_features(df):\n",
    "    \"\"\"\n",
    "    Create time-based features for load forecasting\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Extract time components\n",
    "    df['hour'] = df.timestamp.dt.hour\n",
    "    df['day_of_week'] = df.timestamp.dt.dayofweek\n",
    "    df['day_of_year'] = df.timestamp.dt.dayofyear\n",
    "    df['month'] = df.timestamp.dt.month\n",
    "    df['quarter'] = df.timestamp.dt.quarter\n",
    "    df['year'] = df.timestamp.dt.year\n",
    "    \n",
    "    # Cyclical encoding for periodic features\n",
    "    df['hour_sin'] = np.sin(2 * np.pi * df.hour / 24)\n",
    "    df['hour_cos'] = np.cos(2 * np.pi * df.hour / 24)\n",
    "    \n",
    "    df['day_sin'] = np.sin(2 * np.pi * df.day_of_week / 7)\n",
    "    df['day_cos'] = np.cos(2 * np.pi * df.day_of_week / 7)\n",
    "    \n",
    "    df['month_sin'] = np.sin(2 * np.pi * df.month / 12)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df.month / 12)\n",
    "    \n",
    "    # Binary features\n",
    "    df['is_weekend'] = (df.day_of_week >= 5).astype(int)\n",
    "    df['is_summer'] = df.month.isin([6, 7, 8]).astype(int)\n",
    "    df['is_winter'] = df.month.isin([12, 1, 2]).astype(int)\n",
    "    \n",
    "    # Peak hours (typically 9-11 AM and 6-9 PM)\n",
    "    df['is_peak_morning'] = df.hour.isin([9, 10, 11]).astype(int)\n",
    "    df['is_peak_evening'] = df.hour.isin([18, 19, 20, 21]).astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_lag_features(df, target_col='load_mw', lags=[1, 2, 3, 24, 48, 168]):\n",
    "    \"\"\"\n",
    "    Create lag features for time series forecasting\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    for lag in lags:\n",
    "        df[f'{target_col}_lag_{lag}'] = df[target_col].shift(lag)\n",
    "    \n",
    "    # Rolling statistics\n",
    "    df[f'{target_col}_rolling_mean_24'] = df[target_col].rolling(window=24, min_periods=1).mean()\n",
    "    df[f'{target_col}_rolling_std_24'] = df[target_col].rolling(window=24, min_periods=1).std()\n",
    "    df[f'{target_col}_rolling_mean_168'] = df[target_col].rolling(window=168, min_periods=1).mean()\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_weather_features(df):\n",
    "    \"\"\"\n",
    "    Create weather-derived features\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Temperature-based features\n",
    "    df['temp_squared'] = df.temperature ** 2\n",
    "    df['temp_deviation_from_comfort'] = np.abs(df.temperature - 22)  # 22°C as comfort zone\n",
    "    \n",
    "    # Heat index approximation\n",
    "    df['heat_index'] = df.temperature + 0.5 * df.humidity\n",
    "    \n",
    "    # Weather categories\n",
    "    df['temp_category'] = pd.cut(df.temperature, bins=[-float('inf'), 10, 20, 30, float('inf')], \n",
    "                                labels=['Cold', 'Cool', 'Warm', 'Hot'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply feature engineering\n",
    "df_features = create_time_features(df)\n",
    "df_features = create_lag_features(df_features)\n",
    "df_features = create_weather_features(df_features)\n",
    "\n",
    "print(f\"Original features: {df.shape[1]}\")\n",
    "print(f\"After feature engineering: {df_features.shape[1]}\")\n",
    "print(\"\\nNew features created:\")\n",
    "new_features = set(df_features.columns) - set(df.columns)\n",
    "print(list(new_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Imputation and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Check missing values after feature engineering\n",
    "print(\"Missing values after feature engineering:\")\n",
    "missing_summary = df_features.isnull().sum()\n",
    "print(missing_summary[missing_summary > 0])\n",
    "\n",
    "# Separate numerical and categorical columns\n",
    "numerical_cols = df_features.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = ['region', 'temp_category']\n",
    "\n",
    "print(f\"\\nNumerical columns: {len(numerical_cols)}\")\n",
    "print(f\"Categorical columns: {len(categorical_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Imputation strategies\n",
    "def impute_missing_values(df, numerical_cols, categorical_cols):\n",
    "    \"\"\"\n",
    "    Apply different imputation strategies for missing values\n",
    "    \"\"\"\n",
    "    df_imputed = df.copy()\n",
    "    \n",
    "    # For numerical columns - use KNN imputation for weather features\n",
    "    weather_cols = ['temperature', 'humidity', 'wind_speed']\n",
    "    weather_cols_present = [col for col in weather_cols if col in df_imputed.columns]\n",
    "    \n",
    "    if weather_cols_present:\n",
    "        knn_imputer = KNNImputer(n_neighbors=5)\n",
    "        df_imputed[weather_cols_present] = knn_imputer.fit_transform(df_imputed[weather_cols_present])\n",
    "    \n",
    "    # For other numerical columns with missing values - use median\n",
    "    other_numerical = [col for col in numerical_cols if col not in weather_cols_present]\n",
    "    \n",
    "    for col in other_numerical:\n",
    "        if df_imputed[col].isnull().sum() > 0:\n",
    "            median_imputer = SimpleImputer(strategy='median')\n",
    "            df_imputed[[col]] = median_imputer.fit_transform(df_imputed[[col]])\n",
    "    \n",
    "    # For categorical columns - use mode\n",
    "    for col in categorical_cols:\n",
    "        if col in df_imputed.columns and df_imputed[col].isnull().sum() > 0:\n",
    "            mode_imputer = SimpleImputer(strategy='most_frequent')\n",
    "            df_imputed[[col]] = mode_imputer.fit_transform(df_imputed[[col]])\n",
    "    \n",
    "    return df_imputed\n",
    "\n",
    "# Apply imputation\n",
    "df_clean = impute_missing_values(df_features, numerical_cols, categorical_cols)\n",
    "\n",
    "# Verify no missing values remain\n",
    "print(\"Missing values after imputation:\")\n",
    "print(df_clean.isnull().sum().sum())\n",
    "\n",
    "# Recalculate weather features after imputation\n",
    "df_clean = create_weather_features(df_clean)\n",
    "\n",
    "print(\"Data imputation completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Encode categorical variables\n",
    "def encode_categorical_features(df, categorical_cols):\n",
    "    \"\"\"\n",
    "    Encode categorical features using appropriate methods\n",
    "    \"\"\"\n",
    "    df_encoded = df.copy()\n",
    "    \n",
    "    # One-hot encoding for region\n",
    "    if 'region' in df_encoded.columns:\n",
    "        region_dummies = pd.get_dummies(df_encoded['region'], prefix='region')\n",
    "        df_encoded = pd.concat([df_encoded, region_dummies], axis=1)\n",
    "        df_encoded.drop('region', axis=1, inplace=True)\n",
    "    \n",
    "    # Label encoding for ordinal categorical variables\n",
    "    if 'temp_category' in df_encoded.columns:\n",
    "        le = LabelEncoder()\n",
    "        df_encoded['temp_category_encoded'] = le.fit_transform(df_encoded['temp_category'].astype(str))\n",
    "        df_encoded.drop('temp_category', axis=1, inplace=True)\n",
    "    \n",
    "    return df_encoded\n",
    "\n",
    "# Apply encoding\n",
    "df_encoded = encode_categorical_features(df_clean, categorical_cols)\n",
    "\n",
    "print(f\"Shape after encoding: {df_encoded.shape}\")\n",
    "print(\"Data preprocessing completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Splitting and Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Prepare data for modeling\n",
    "# Remove timestamp and target variable from features\n",
    "feature_cols = [col for col in df_encoded.columns if col not in ['timestamp', 'load_mw']]\n",
    "target_col = 'load_mw'\n",
    "\n",
    "# Remove rows with NaN values (mainly from lag features)\n",
    "df_model = df_encoded.dropna().copy()\n",
    "\n",
    "X = df_model[feature_cols]\n",
    "y = df_model[target_col]\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Target vector shape: {y.shape}\")\n",
    "print(f\"\\nFeatures used: {len(feature_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Time series split for proper validation\n",
    "# Use the last 20% of data for testing\n",
    "split_index = int(0.8 * len(df_model))\n",
    "\n",
    "X_train = X.iloc[:split_index]\n",
    "X_test = X.iloc[split_index:]\n",
    "y_train = y.iloc[:split_index]\n",
    "y_test = y.iloc[split_index:]\n",
    "\n",
    "# Get corresponding timestamps for analysis\n",
    "train_timestamps = df_model['timestamp'].iloc[:split_index]\n",
    "test_timestamps = df_model['timestamp'].iloc[split_index:]\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "print(f\"Training period: {train_timestamps.min()} to {train_timestamps.max()}\")\n",
    "print(f\"Test period: {test_timestamps.min()} to {test_timestamps.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert back to DataFrames for easier handling\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=feature_cols, index=X_train.index)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=feature_cols, index=X_test.index)\n",
    "\n",
    "print(\"Feature scaling completed!\")\n",
    "print(f\"Feature means after scaling: {X_train_scaled.mean().round(3).head()}\")\n",
    "print(f\"Feature stds after scaling: {X_train_scaled.std().round(3).head()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Define evaluation metrics\n",
    "def evaluate_model(y_true, y_pred, model_name):\n",
    "    \"\"\"\n",
    "    Calculate and display evaluation metrics\n",
    "    \"\"\"\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "    \n",
    "    print(f\"\\n{model_name} Performance:\")\n",
    "    print(f\"MAE: {mae:.2f} MW\")\n",
    "    print(f\"RMSE: {rmse:.2f} MW\")\n",
    "    print(f\"R²: {r2:.4f}\")\n",
    "    print(f\"MAPE: {mape:.2f}%\")\n",
    "    \n",
    "    return {'MAE': mae, 'RMSE': rmse, 'R2': r2, 'MAPE': mape}\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "# Train and evaluate models\n",
    "model_results = {}\n",
    "trained_models = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    # Train model\n",
    "    if name == 'Linear Regression':\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Evaluate model\n",
    "    results = evaluate_model(y_test, y_pred, name)\n",
    "    model_results[name] = results\n",
    "    trained_models[name] = (model, y_pred)\n",
    "\n",
    "print(\"\\nModel training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Cross-validation with time series split\n",
    "def time_series_cv(X, y, model, n_splits=5):\n",
    "    \"\"\"\n",
    "    Perform time series cross-validation\n",
    "    \"\"\"\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    cv_scores = []\n",
    "    \n",
    "    for train_idx, val_idx in tscv.split(X):\n",
    "        X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        \n",
    "        model.fit(X_tr, y_tr)\n",
    "        y_pred = model.predict(X_val)\n",
    "        \n",
    "        mae = mean_absolute_error(y_val, y_pred)\n",
    "        cv_scores.append(mae)\n",
    "    \n",
    "    return cv_scores\n",
    "\n",
    "# Perform cross-validation for Random Forest (best performing model)\n",
    "print(\"Performing time series cross-validation...\")\n",
    "rf_cv_scores = time_series_cv(X_train, y_train, RandomForestRegressor(n_estimators=100, random_state=42))\n",
    "\n",
    "print(f\"Cross-validation MAE scores: {[f'{score:.2f}' for score in rf_cv_scores]}\")\n",
    "print(f\"Mean CV MAE: {np.mean(rf_cv_scores):.2f} ± {np.std(rf_cv_scores):.2f} MW\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Evaluation and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Comparison of model performance\n",
    "results_df = pd.DataFrame(model_results).T\n",
    "print(\"Model Performance Comparison:\")\n",
    "print(results_df)\n",
    "\n",
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "metrics = ['MAE', 'RMSE', 'R2', 'MAPE']\n",
    "for i, metric in enumerate(metrics):\n",
    "    ax = axes[i//2, i%2]\n",
    "    results_df[metric].plot(kind='bar', ax=ax, color=['skyblue', 'lightgreen', 'lightcoral'])\n",
    "    ax.set_title(f'{metric} Comparison')\n",
    "    ax.set_ylabel(metric)\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Detailed visualization of predictions\n",
    "best_model_name = 'Random Forest'  # Based on typical performance\n",
    "best_model, best_predictions = trained_models[best_model_name]\n",
    "\n",
    "# Time series plot of predictions vs actual\n",
    "fig, axes = plt.subplots(3, 1, figsize=(15, 12))\n",
    "\n",
    "# Full test period\n",
    "axes[0].plot(test_timestamps, y_test.values, label='Actual', alpha=0.8)\n",
    "axes[0].plot(test_timestamps, best_predictions, label='Predicted', alpha=0.8)\n",
    "axes[0].set_title(f'{best_model_name} Predictions vs Actual - Full Test Period')\n",
    "axes[0].set_ylabel('Load (MW)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Zoomed view (first week)\n",
    "week_mask = test_timestamps <= test_timestamps.min() + timedelta(days=7)\n",
    "axes[1].plot(test_timestamps[week_mask], y_test[week_mask], label='Actual', marker='o', markersize=2)\n",
    "axes[1].plot(test_timestamps[week_mask], best_predictions[week_mask], label='Predicted', marker='s', markersize=2)\n",
    "axes[1].set_title('First Week - Detailed View')\n",
    "axes[1].set_ylabel('Load (MW)')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Residuals\n",
    "residuals = y_test.values - best_predictions\n",
    "axes[2].plot(test_timestamps, residuals, alpha=0.7, color='red')\n",
    "axes[2].axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "axes[2].set_title('Prediction Residuals')\n",
    "axes[2].set_ylabel('Residual (MW)')\n",
    "axes[2].set_xlabel('Time')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Scatter plot and residual analysis\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Predicted vs Actual scatter plot\n",
    "axes[0,0].scatter(y_test, best_predictions, alpha=0.6)\n",
    "axes[0,0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "axes[0,0].set_xlabel('Actual Load (MW)')\n",
    "axes[0,0].set_ylabel('Predicted Load (MW)')\n",
    "axes[0,0].set_title('Predicted vs Actual')\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Residuals distribution\n",
    "axes[0,1].hist(residuals, bins=50, alpha=0.7, edgecolor='black')\n",
    "axes[0,1].set_xlabel('Residuals (MW)')\n",
    "axes[0,1].set_ylabel('Frequency')\n",
    "axes[0,1].set_title('Residuals Distribution')\n",
    "axes[0,1].axvline(x=0, color='red', linestyle='--')\n",
    "\n",
    "# QQ plot for residuals normality\n",
    "from scipy import stats\n",
    "stats.probplot(residuals, dist=\"norm\", plot=axes[1,0])\n",
    "axes[1,0].set_title('Q-Q Plot of Residuals')\n",
    "\n",
    "# Residuals vs predicted values\n",
    "axes[1,1].scatter(best_predictions, residuals, alpha=0.6)\n",
    "axes[1,1].axhline(y=0, color='red', linestyle='--')\n",
    "axes[1,1].set_xlabel('Predicted Load (MW)')\n",
    "axes[1,1].set_ylabel('Residuals (MW)')\n",
    "axes[1,1].set_title('Residuals vs Predicted')\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Feature importance for Random Forest\n",
    "if best_model_name == 'Random Forest':\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': feature_cols,\n",
    "        'importance': best_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    # Plot top 20 most important features\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    top_features = feature_importance.head(20)\n",
    "    \n",
    "    plt.barh(range(len(top_features)), top_features['importance'], \n",
    "             color=plt.cm.viridis(np.linspace(0, 1, len(top_features))))\n",
    "    plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "    plt.xlabel('Feature Importance')\n",
    "    plt.title('Top 20 Most Important Features (Random Forest)')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Top 10 Most Important Features:\")\n",
    "    print(feature_importance.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Inference with New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Create a function for making predictions on new data\n",
    "def predict_load(model, scaler, new_data, model_name, feature_cols):\n",
    "    \"\"\"\n",
    "    Make load predictions on new data\n",
    "    \"\"\"\n",
    "    # Ensure new_data has all required features\n",
    "    for col in feature_cols:\n",
    "        if col not in new_data.columns:\n",
    "            print(f\"Warning: Missing feature {col}, setting to 0\")\n",
    "            new_data[col] = 0\n",
    "    \n",
    "    # Select and order features correctly\n",
    "    X_new = new_data[feature_cols]\n",
    "    \n",
    "    # Scale if needed (for Linear Regression)\n",
    "    if model_name == 'Linear Regression':\n",
    "        X_new_scaled = scaler.transform(X_new)\n",
    "        predictions = model.predict(X_new_scaled)\n",
    "    else:\n",
    "        predictions = model.predict(X_new)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# Generate future data for demonstration\n",
    "def generate_future_data(start_date, periods=168):  # 1 week of hourly data\n",
    "    \"\"\"\n",
    "    Generate future data with weather forecasts for demonstration\n",
    "    \"\"\"\n",
    "    future_dates = pd.date_range(start=start_date, periods=periods, freq='H')\n",
    "    \n",
    "    # Simulate weather forecast (with some uncertainty)\n",
    "    base_temp = 25 + 10 * np.sin(2 * np.pi * future_dates.dayofyear / 365.25)\n",
    "    temperature = base_temp + np.random.normal(0, 2, periods)  # Less variation in forecast\n",
    "    \n",
    "    humidity = 60 + 20 * np.sin(2 * np.pi * future_dates.dayofyear / 365.25) + np.random.normal(0, 5, periods)\n",
    "    humidity = np.clip(humidity, 20, 95)\n",
    "    \n",
    "    wind_speed = 8 + 5 * np.sin(2 * np.pi * future_dates.dayofyear / 365.25) + np.random.exponential(1.5, periods)\n",
    "    wind_speed = np.clip(wind_speed, 0, 25)\n",
    "    \n",
    "    future_df = pd.DataFrame({\n",
    "        'timestamp': future_dates,\n",
    "        'temperature': temperature,\n",
    "        'humidity': humidity,\n",
    "        'wind_speed': wind_speed,\n",
    "        'region': np.random.choice(['North', 'South', 'Central', 'Northeast'], periods)\n",
    "    })\n",
    "    \n",
    "    return future_df\n",
    "\n",
    "# Generate future scenario\n",
    "future_start = test_timestamps.max() + timedelta(hours=1)\n",
    "future_data = generate_future_data(future_start)\n",
    "\n",
    "print(f\"Generated future data for prediction:\")\n",
    "print(f\"Period: {future_data.timestamp.min()} to {future_data.timestamp.max()}\")\n",
    "print(f\"Shape: {future_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Prepare future data for prediction\n",
    "def prepare_future_data_for_prediction(future_data, last_known_load_values):\n",
    "    \"\"\"\n",
    "    Prepare future data by adding all necessary features\n",
    "    Note: In practice, lag features would need special handling\n",
    "    \"\"\"\n",
    "    # Add time features\n",
    "    future_processed = create_time_features(future_data)\n",
    "    \n",
    "    # Add weather features\n",
    "    future_processed = create_weather_features(future_processed)\n",
    "    \n",
    "    # Encode categorical features\n",
    "    future_processed = encode_categorical_features(future_processed, ['region', 'temp_category'])\n",
    "    \n",
    "    # For lag features, we'll use the last known values as approximation\n",
    "    # In practice, you'd need a more sophisticated approach for multi-step prediction\n",
    "    lag_features = [col for col in feature_cols if 'lag' in col or 'rolling' in col]\n",
    "    \n",
    "    for feature in lag_features:\n",
    "        if 'lag_1' in feature:\n",
    "            future_processed[feature] = last_known_load_values[-1]\n",
    "        elif 'lag_2' in feature:\n",
    "            future_processed[feature] = last_known_load_values[-2] if len(last_known_load_values) > 1 else last_known_load_values[-1]\n",
    "        elif 'lag_3' in feature:\n",
    "            future_processed[feature] = last_known_load_values[-3] if len(last_known_load_values) > 2 else last_known_load_values[-1]\n",
    "        elif 'lag_24' in feature:\n",
    "            future_processed[feature] = np.mean(last_known_load_values[-24:]) if len(last_known_load_values) >= 24 else last_known_load_values[-1]\n",
    "        elif 'rolling_mean_24' in feature:\n",
    "            future_processed[feature] = np.mean(last_known_load_values[-24:]) if len(last_known_load_values) >= 24 else np.mean(last_known_load_values)\n",
    "        elif 'rolling_std_24' in feature:\n",
    "            future_processed[feature] = np.std(last_known_load_values[-24:]) if len(last_known_load_values) >= 24 else np.std(last_known_load_values) if len(last_known_load_values) > 1 else 0\n",
    "        elif 'rolling_mean_168' in feature:\n",
    "            future_processed[feature] = np.mean(last_known_load_values[-168:]) if len(last_known_load_values) >= 168 else np.mean(last_known_load_values)\n",
    "        else:\n",
    "            future_processed[feature] = np.mean(last_known_load_values[-48:]) if len(last_known_load_values) >= 48 else np.mean(last_known_load_values)\n",
    "    \n",
    "    return future_processed\n",
    "\n",
    "# Get last known load values from test set\n",
    "last_known_loads = y_test.values[-168:]  # Last week of known data\n",
    "\n",
    "# Prepare future data\n",
    "future_prepared = prepare_future_data_for_prediction(future_data, last_known_loads)\n",
    "\n",
    "# Make predictions\n",
    "future_predictions = predict_load(best_model, scaler, future_prepared, best_model_name, feature_cols)\n",
    "\n",
    "print(f\"Generated {len(future_predictions)} load predictions\")\n",
    "print(f\"Predicted load range: {future_predictions.min():.2f} - {future_predictions.max():.2f} MW\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize future predictions\n",
    "fig, axes = plt.subplots(3, 1, figsize=(15, 12))\n",
    "\n",
    "# Historical + Future predictions\n",
    "historical_period = test_timestamps[-168:]  # Last week of test data\n",
    "historical_actual = y_test.iloc[-168:].values\n",
    "historical_pred = best_predictions[-168:]\n",
    "\n",
    "# Plot historical data\n",
    "axes[0].plot(historical_period, historical_actual, label='Historical Actual', linewidth=2)\n",
    "axes[0].plot(historical_period, historical_pred, label='Historical Predicted', linewidth=2, alpha=0.8)\n",
    "\n",
    "# Plot future predictions\n",
    "axes[0].plot(future_data['timestamp'], future_predictions, label='Future Predictions', \n",
    "            linewidth=2, linestyle='--', color='red')\n",
    "axes[0].axvline(x=future_start, color='black', linestyle=':', alpha=0.7, label='Forecast Start')\n",
    "axes[0].set_title('Load Forecasting: Historical Performance and Future Predictions')\n",
    "axes[0].set_ylabel('Load (MW)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Future predictions only\n",
    "axes[1].plot(future_data['timestamp'], future_predictions, marker='o', markersize=3, linewidth=2)\n",
    "axes[1].set_title('Future Load Predictions (Next Week)')\n",
    "axes[1].set_ylabel('Load (MW)')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Weather context for future period\n",
    "ax2 = axes[2]\n",
    "ax3 = ax2.twinx()\n",
    "\n",
    "ax2.plot(future_data['timestamp'], future_data['temperature'], color='red', label='Temperature', linewidth=2)\n",
    "ax3.plot(future_data['timestamp'], future_predictions, color='blue', label='Predicted Load', linewidth=2)\n",
    "\n",
    "ax2.set_ylabel('Temperature (°C)', color='red')\n",
    "ax3.set_ylabel('Load (MW)', color='blue')\n",
    "ax2.set_title('Temperature vs Predicted Load')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Add legends\n",
    "lines1, labels1 = ax2.get_legend_handles_labels()\n",
    "lines2, labels2 = ax3.get_legend_handles_labels()\n",
    "ax2.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary and Key Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Summary statistics and insights\n",
    "print(\"=\" * 60)\n",
    "print(\"REGIONAL LOAD FORECASTING WORKSHOP SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n📊 DATASET OVERVIEW:\")\n",
    "print(f\"• Total samples: {len(df):,}\")\n",
    "print(f\"• Time period: {df.timestamp.min().strftime('%Y-%m-%d')} to {df.timestamp.max().strftime('%Y-%m-%d')}\")\n",
    "print(f\"• Features created: {len(feature_cols)}\")\n",
    "print(f\"• Training samples: {len(X_train):,}\")\n",
    "print(f\"• Test samples: {len(X_test):,}\")\n",
    "\n",
    "print(\"\\n🎯 MODEL PERFORMANCE:\")\n",
    "for model_name, metrics in model_results.items():\n",
    "    print(f\"• {model_name}:\")\n",
    "    print(f\"  - MAE: {metrics['MAE']:.2f} MW\")\n",
    "    print(f\"  - RMSE: {metrics['RMSE']:.2f} MW\")\n",
    "    print(f\"  - R²: {metrics['R2']:.4f}\")\n",
    "    print(f\"  - MAPE: {metrics['MAPE']:.2f}%\")\n",
    "\n",
    "print(f\"\\n🏆 BEST MODEL: {best_model_name}\")\n",
    "best_results = model_results[best_model_name]\n",
    "print(f\"• Accuracy: {best_results['R2']:.1%}\")\n",
    "print(f\"• Average error: {best_results['MAE']:.2f} MW\")\n",
    "print(f\"• Error percentage: {best_results['MAPE']:.2f}%\")\n",
    "\n",
    "print(\"\\n🔍 KEY INSIGHTS:\")\n",
    "print(\"• Load patterns show strong daily and weekly seasonality\")\n",
    "print(\"• Temperature is a major driver of electricity demand\")\n",
    "print(\"• Historical load values (lag features) are highly predictive\")\n",
    "print(\"• Peak demand occurs during extreme weather conditions\")\n",
    "print(\"• Weekdays generally have higher demand than weekends\")\n",
    "\n",
    "if best_model_name == 'Random Forest':\n",
    "    print(\"\\n🌟 TOP PREDICTIVE FEATURES:\")\n",
    "    for i, (feature, importance) in enumerate(feature_importance.head(5).values):\n",
    "        print(f\"• {i+1}. {feature}: {importance:.4f}\")\n",
    "\n",
    "print(\"\\n🚀 FUTURE PREDICTIONS:\")\n",
    "print(f\"• Forecast period: {future_data.timestamp.min().strftime('%Y-%m-%d %H:%M')} to {future_data.timestamp.max().strftime('%Y-%m-%d %H:%M')}\")\n",
    "print(f\"• Predicted load range: {future_predictions.min():.0f} - {future_predictions.max():.0f} MW\")\n",
    "print(f\"• Average predicted load: {future_predictions.mean():.0f} MW\")\n",
    "\n",
    "print(\"\\n💡 RECOMMENDATIONS:\")\n",
    "print(\"• Monitor weather forecasts for load planning\")\n",
    "print(\"• Consider regional differences in load patterns\")\n",
    "print(\"• Update models regularly with new data\")\n",
    "print(\"• Implement ensemble methods for improved accuracy\")\n",
    "print(\"• Add external factors (holidays, economic indicators)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nakara-skybound-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
